{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07d34e7",
   "metadata": {},
   "source": [
    "# Predicting Student Exam Scores Using a Deep Learning model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kaggle as kg\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure we are in the right directory\n",
    "def read_data(test_or_train='train'):\n",
    "    if 'data' in os.listdir():\n",
    "        train_df = pd.read_csv(f'data/playground-series-s6e1/{test_or_train}.csv')\n",
    "    else:\n",
    "        os.chdir('..')\n",
    "        train_df = pd.read_csv(f'data/playground-series-s6e1/{test_or_train}.csv')\n",
    "    return train_df\n",
    "\n",
    "TARGET_COL = 'exam_score'\n",
    "ID_COL = 'id'\n",
    "\n",
    "def create_and_fit_preprocessor(df_train, target_col=TARGET_COL, id_col=ID_COL):\n",
    "    \"\"\"\n",
    "    Creates a preprocessor and teaches it (fits it) using the training data.\n",
    "    Returns the fitted preprocessor.\n",
    "    \"\"\"\n",
    "    # 1. Identify columns\n",
    "    # Note: We drop target/ID first so we don't accidentally scale them\n",
    "    X_train = df_train.drop(columns=[id_col, target_col])\n",
    "    \n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "    # 2. Define the preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            # handle_unknown='ignore' is crucial for test data!\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "        ],\n",
    "        verbose_feature_names_out=False # Keeps column names clean\n",
    "    )\n",
    "\n",
    "    # 3. FIT the preprocessor on training data\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def process_data(df, preprocessor, target_col=TARGET_COL, id_col=ID_COL):\n",
    "    \"\"\"\n",
    "    Uses an EXISTING preprocessor to transform data.\n",
    "    Does not learn new patterns; simply applies existing rules.\n",
    "    \"\"\"\n",
    "    # Remove ID and Target if they exist (so we only transform features)\n",
    "    # We use 'errors=ignore' so this works on test data (which might not have a score column)\n",
    "    drop_cols = [c for c in [id_col, target_col] if c in df.columns]\n",
    "    X = df.drop(columns=drop_cols, errors='ignore')\n",
    "    \n",
    "    # 4. TRANSFORM only (do not fit!)\n",
    "    X_processed = preprocessor.transform(X)\n",
    "    \n",
    "    # Optional: Convert back to DataFrame for readability (preserves column names)\n",
    "    # This requires scikit-learn v1.0+\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        return pd.DataFrame(X_processed, columns=feature_names)\n",
    "    except:\n",
    "        return pd.DataFrame(X_processed) \n",
    "\n",
    "#Dataloader:\n",
    "def create_dataloader(features, targets=None, batch_size=128, shuffle=True):\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    if targets is None:\n",
    "        dataset = TensorDataset(features_tensor)\n",
    "    else:\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.float32)\n",
    "        dataset = TensorDataset(features_tensor, targets_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader\n",
    "\n",
    "train_df = read_data('train')\n",
    "test_df = read_data('test')\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "preprocessor = create_and_fit_preprocessor(train_df)\n",
    "train_processed = process_data(train_df, preprocessor)\n",
    "val_processed = process_data(val_df, preprocessor)\n",
    "test_processed = process_data(test_df, preprocessor)\n",
    "\n",
    "y_train = train_df[TARGET_COL].astype(np.float32).to_numpy()\n",
    "y_val = val_df[TARGET_COL].astype(np.float32).to_numpy()\n",
    "\n",
    "train_dataloader = create_dataloader(train_processed.values, y_train)\n",
    "val_dataloader = create_dataloader(val_processed.values, y_val, shuffle=False)\n",
    "test_dataloader = create_dataloader(test_processed.values, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42df50",
   "metadata": {},
   "source": [
    "## Build model:\n",
    "\n",
    "I decided to use a simple architecture for the model, a MLP with 4 layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ad69df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkModel(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetworkModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, 32)\n",
    "        self.fc5 = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "\n",
    "        #Add dropout for generalization\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1eb090",
   "metadata": {},
   "source": [
    "### Define Function for training model using tqdm to keep track of progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc65e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model, keep track of loss with tqdm progress bar.\n",
    "#Save Validation and train losses for each epoch.\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "\n",
    "#TODO: Save train and val losses for each epoch.\n",
    "def train_model(model, dataloader_train, dataloader_val, criterion, optimizer, device, epochs=10):\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader_train.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader_val:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        val_epoch_loss = val_loss / len(dataloader_val.dataset)\n",
    "        print(f\"Validation Loss: {val_epoch_loss:.4f}\")\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        model.train()\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91091f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "input_size = train_processed.shape[1]\n",
    "model = NeuralNetworkModel(input_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_hist = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255b791",
   "metadata": {},
   "source": [
    "## Plot train and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot train and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_hist['train_loss'], label='Train Loss')\n",
    "plt.plot(train_hist['val_loss'], label='Validation Loss')   \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e023eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_dataloader:\n",
    "            inputs = inputs[0].to(device)\n",
    "            predictions.append(model(inputs).squeeze().cpu().numpy())\n",
    "    return predictions\n",
    "\n",
    "test_predictions = test_model(model, test_dataloader, device)\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "#Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    ID_COL: test_df[ID_COL],\n",
    "    TARGET_COL: test_predictions\n",
    "})\n",
    "submission_df.to_csv('nn_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
